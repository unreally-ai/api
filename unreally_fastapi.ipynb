{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unreally-ai/api/blob/main/unreally_fastapi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psIk6EXjI-IJ",
        "outputId": "caf8b6a1-fbdc-4b8b-a283-47c8d89889c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.7/dist-packages (0.82.0)\n",
            "Requirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /usr/local/lib/python3.7/dist-packages (from fastapi) (1.9.2)\n",
            "Requirement already satisfied: starlette==0.19.1 in /usr/local/lib/python3.7/dist-packages (from fastapi) (0.19.1)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.7/dist-packages (from starlette==0.19.1->fastapi) (4.1.1)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from starlette==0.19.1->fastapi) (3.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.7/dist-packages (from anyio<5,>=3.4.0->starlette==0.19.1->fastapi) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.7/dist-packages (from anyio<5,>=3.4.0->starlette==0.19.1->fastapi) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.2.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch\n",
            "  Using cached pytorch-1.0.2.tar.gz (689 bytes)\n",
            "Building wheels for collected packages: pytorch\n",
            "  Building wheel for pytorch (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for pytorch\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for pytorch\n",
            "Failed to build pytorch\n",
            "Installing collected packages: pytorch\n",
            "    Running setup.py install for pytorch ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-h8h1mwx_/pytorch_c0f308ef464145e1bfb6fe85ca393323/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-h8h1mwx_/pytorch_c0f308ef464145e1bfb6fe85ca393323/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-xndej4zg/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.7/pytorch Check the logs for full command output.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.7.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi\n",
        "!pip install requests\n",
        "!pip install pandas\n",
        "!pip install pytorch\n",
        "!pip install -U scikit-learn\n",
        "!pip install --user -U nltk\n",
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "k_tCZUCZKxdz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a16b8908-875d-48f6-cdae-4f9f58d7ac4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (1.5.5)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.7/dist-packages (5.1.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok) (6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nest-asyncio\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5BIz5YvRMwMV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17d98725-1294-4fbb-e6c0-ac641d91c709"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.7/dist-packages (0.18.3)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from uvicorn) (7.1.2)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.7/dist-packages (from uvicorn) (0.13.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from uvicorn) (4.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install uvicorn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8UmbduDKiXH"
      },
      "source": [
        "News API imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFYMZL9KKmiq",
        "outputId": "76243244-6149-4a37-85b6-541bc738a936"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# -------- API ----------\n",
        "from fastapi import FastAPI\n",
        "\n",
        "# -------- News API ----------\n",
        "import requests\n",
        "\n",
        "# ------- Machine Learning ---------\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from sklearn.feature_extraction import text\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# ---------------- LETS GET TO THE CODE ---------------------\n",
        "\n",
        "# Create News API Client\n",
        "URL = \"https://rapidapi.p.rapidapi.com/api/search/NewsSearchAPI\"\n",
        "HEADERS = {\n",
        "    \"x-rapidapi-host\": \"contextualwebsearch-websearch-v1.p.rapidapi.com\",\n",
        "    \"x-rapidapi-key\": \"628b9c410bmsh539c4bc65a125b5p1b57bbjsn97e9e4dc1b28\"\n",
        "}\n",
        "\n",
        "# ----------------------------- PARAMETERS -----------------------------\n",
        "CUSTOM_SW = [\"semst\",\"u\"] # TODO: add to actual stopwords\n",
        "VOCAB = \"kowalsky_vocab.csv\"\n",
        "USE_LEMMATIZER = True\n",
        "\n",
        "# TODO: select headlines & body out of dataset \n",
        "vocab_df = pd.read_csv(VOCAB, header=None)\n",
        "\n",
        "# ----------------------------- BOW VECTORIZER PIPELINE -----------------------------\n",
        "# takes [string], returns lowercased & lemmatized [string]\n",
        "def lem_str(in_string):\n",
        "    out_str = [\"\"]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    for word in in_string.split():\n",
        "        out_str[0] += (lemmatizer.lemmatize(word.lower()) + \" \")\n",
        "    \n",
        "    return out_str\n",
        "\n",
        "# takes vocab & returns bow_vectorizer\n",
        "def load_vectorizer(path):\n",
        "    # define stopwords\n",
        "    sw = text.ENGLISH_STOP_WORDS.union([\"book\"])\n",
        "    # read vocabulary\n",
        "    vocab_df = pd.read_csv(path, header=None)\n",
        "    vocab_df = vocab_df.drop([0])\n",
        "\n",
        "    bow_vectorizer = CountVectorizer(\n",
        "        stop_words=sw,\n",
        "        max_features=5000,\n",
        "        vocabulary=vocab_df[1]\n",
        "    )\n",
        "\n",
        "\n",
        "    return bow_vectorizer\n",
        "\n",
        "# takes string & path to vocab and yeets it through the BoW pipeline\n",
        "def create_bow(in_string, path):\n",
        "    bow_vectorizer = load_vectorizer(path)\n",
        "    if USE_LEMMATIZER == True:\n",
        "        bow = bow_vectorizer.fit_transform(lem_str(in_string), y=None)\n",
        "    else:\n",
        "        bow = bow_vectorizer.fit_transform([in_string], y=None)\n",
        "\n",
        "    return bow\n",
        "\n",
        "\n",
        "# ----------------------------- TF-IDF PIPELINE -----------------------------\n",
        "\n",
        "def create_tf(bow_vec):\n",
        "    tfreq_vec = TfidfTransformer(use_idf=False).fit(bow_vec)\n",
        "    tfreq = tfreq_vec.transform(bow_vec)\n",
        "\n",
        "    return tfreq\n",
        "\n",
        "# TODO figure out\n",
        "\n",
        "def create_tfidf(bow):\n",
        "    tfreq_vec = TfidfTransformer(use_idf=True)\n",
        "    tfreq = tfreq_vec.fit_transform(bow)\n",
        "\n",
        "    return tfreq\n",
        "\n",
        "# -------------- GENEREATE 10001 VECTOR --------------\n",
        "def yeet2vec(head, body):\n",
        "\n",
        "    # get our sub-vectores\n",
        "    claim_tf = create_tf(create_bow(head, VOCAB))\n",
        "    body_tf = create_tf(create_bow(body, VOCAB))\n",
        "    \n",
        "    claim_tfidf = create_tfidf(create_bow(head, VOCAB))\n",
        "    body_tfidf = create_tfidf(create_bow(body, VOCAB))\n",
        "\n",
        "    print(\"  - created sub-vectors ✅\")\n",
        "\n",
        "    # tasty cosine similarity\n",
        "    c_sim = cosine_similarity(claim_tfidf ,body_tfidf)\n",
        "\n",
        "    # do the yeeting\n",
        "    # HERE IS SOME ERRROR WITH CONCAT\n",
        "    claim_df = pd.DataFrame(claim_tf.toarray()) \n",
        "    body_df = pd.DataFrame(body_tf.toarray())\n",
        "    c_sim_df = pd.DataFrame(c_sim)\n",
        "    \n",
        "    tenk = pd.concat([claim_df,c_sim_df,body_df],axis=1)\n",
        "    tenk = tenk.to_numpy()\n",
        "    tenk = torch.from_numpy(tenk)\n",
        "    terror = [tenk]\n",
        "\n",
        "    print(\"  - created 10k vector ✅\")\n",
        "    return tenk\n",
        "\n",
        "# load the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
        "\n",
        "# set dimensions\n",
        "in_dim = 10001\n",
        "hidden_dim = 100\n",
        "out_dim = 4\n",
        "\n",
        "class NN(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
        "        super(NN, self).__init__()\n",
        "        # define layers\n",
        "        self.l1 = nn.Linear(in_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.l2 = nn.Linear(hidden_dim, out_dim)\n",
        "    \n",
        "    # applies layers with sample x\n",
        "    def forward(self, x):\n",
        "        out = self.l1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.l2(out)\n",
        "        return out\n",
        "\n",
        "model = NN(in_dim, hidden_dim, out_dim)\n",
        "model.load_state_dict(torch.load('kowalsky_72_balanced.pth', map_location=torch.device('cpu')))\n",
        "model.eval()\n",
        "\n",
        "def predict(tenk_vec):\n",
        "    with torch.no_grad():\n",
        "        pred = model(tenk_vec.float())\n",
        "        pred_out, pred_idx = torch.max(pred, 1)\n",
        "\n",
        "\n",
        "    classes = ['agree', 'disagree', 'discuss', 'unrelated']\n",
        "    print(pred)\n",
        "    stance = classes[((pred_idx.data).numpy()[0])]\n",
        "    return dict(zip(classes, pred.tolist()[0])), stance\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def route():\n",
        "    return {\"message\":\"Willkommen zu der Unreally API\"}\n",
        "\n",
        "@app.get(\"/predict\")\n",
        "async def use_model(query: str):\n",
        "    # get bodies matching test_string\n",
        "    test_string = query\n",
        "    # get bodies matching test_string\n",
        "    page_number = 1\n",
        "    page_size = 5\n",
        "    auto_correct = True\n",
        "    safe_search = True\n",
        "    with_thumbnails = False\n",
        "    from_published_date = \"\"\n",
        "    to_published_date = \"\"\n",
        "\n",
        "    querystring = {\"q\": test_string,\n",
        "                \"pageNumber\": page_number,\n",
        "                \"pageSize\": page_size,\n",
        "                \"autoCorrect\": auto_correct,\n",
        "                \"safeSearch\": safe_search,\n",
        "                \"withThumbnails\": with_thumbnails,\n",
        "                \"fromPublishedDate\": from_published_date,\n",
        "                \"toPublishedDate\": to_published_date}\n",
        "\n",
        "    response = requests.get(URL, headers=HEADERS, params=querystring).json()\n",
        "    test_body = \"\" # initiate test_body (the body we need to test the stance against the test_string)\n",
        "    sources = []\n",
        "    print(len(response['value']))\n",
        "    for article in response['value']:\n",
        "        body = article['body']\n",
        "        sources.append(article['url'])\n",
        "        test_body += body\n",
        "        vector = yeet2vec(test_string, test_body)\n",
        "        prediction = predict(vector)\n",
        "    if len(test_body) == 0:\n",
        "        return \"No articles found :/\"\n",
        "    # yeets strings through pipeline, outputs finished 10k vector\n",
        "    print(len(test_body))\n",
        "    print(test_body)\n",
        "    vector = yeet2vec(test_string, test_body)\n",
        "    values, prediction = predict(vector)\n",
        "    # send answer tweet\n",
        "    return {'prediction': prediction,'values': values, 'sources': sources}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eeyLXjELv6w",
        "outputId": "6ae7b2cb-2e6c-4554-dad2-d9a6dd68d150"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "auth_token = \"2EcVBvcw5Tp8bPrZfkMMwTKBctH_5vinjaGx4VxZj2a1r5cHs\" #@param {type:\"string\"}\n",
        "# Since we can't access Colab notebooks IP directly we'll use\n",
        "# ngrok to create a public URL for the server via a tunnel\n",
        "\n",
        "import os\n",
        "os.system(f\"ngrok authtoken {auth_token}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8aHuHuIKL5rD"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "public_url = ngrok.connect(8000, port='8000', bind_tls=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0N2HZmCwOE7q",
        "outputId": "71f54f3b-1fb3-44db-8bad-e73e0c3a2ec4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root         707  2.6  0.1 726660 24712 ?        Sl   16:27   0:00 /usr/local/lib/python3.7/dist-packages/pyngrok/bin/ngrok start --none --log=stdout\n",
            "root         717  0.0  0.0  39204  6656 ?        S    16:27   0:00 /bin/bash -c ps aux | grep ngrok\n",
            "root         719  0.0  0.0  38576  5632 ?        R    16:27   0:00 grep ngrok\n"
          ]
        }
      ],
      "source": [
        "!ps aux | grep ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_k4SPLBMRDS",
        "outputId": "0fe8f61e-1aca-468a-d1a1-c9b65e0e41b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NgrokTunnel: \"https://2e0f-34-86-96-151.ngrok.io\" -> \"http://localhost:8000\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [57]\n",
            "INFO:uvicorn.error:Started server process [57]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:uvicorn.error:Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:uvicorn.error:Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n",
            "INFO:uvicorn.error:Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n",
            "INFO:     Shutting down\n",
            "INFO:uvicorn.error:Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:uvicorn.error:Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:uvicorn.error:Application shutdown complete.\n",
            "INFO:     Finished server process [57]\n",
            "INFO:uvicorn.error:Finished server process [57]\n"
          ]
        }
      ],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "# Allow for asyncio to work within the Jupyter notebook cell\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import uvicorn\n",
        "\n",
        "# Run the FastAPI app using uvicorn\n",
        "print(public_url)\n",
        "uvicorn.run(app)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyNANU1H1S5ZJeTcISga1uA/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}